# Nexus - LLM Inference Orchestration Platform

[![Rust](https://img.shields.io/badge/rust-1.75+-orange.svg)](https://www.rust-lang.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/docker-supported-blue.svg)](https://www.docker.com/)

> A high-performance, production-ready LLM inference orchestration platform built in Rust

## ğŸš€ Overview

Nexus is a comprehensive LLMOps platform designed to solve real-world challenges in serving Large Language Models at scale. Built with Rust for maximum performance and safety, Nexus provides enterprise-grade features for model serving, monitoring, and optimization.

### Key Features

- **ğŸ”„ Multi-Model Serving**: Deploy and manage multiple LLM models simultaneously
- **âš¡ Smart Load Balancing**: Intelligent request routing based on model capacity and latency
- **ğŸ“Š Dynamic Batching**: Automatic request batching for optimal throughput
- **ğŸ¯ A/B Testing**: Built-in experimentation framework for model comparisons
- **ğŸ“ˆ Real-time Metrics**: Comprehensive observability with Prometheus integration
- **ğŸ’¾ Intelligent Caching**: Multi-layered caching system for improved response times
- **ğŸ”§ Resource Management**: Automatic scaling and resource optimization
- **ğŸ›¡ï¸ Production Ready**: Health checks, graceful shutdowns, and error handling

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load Balancer â”‚â”€â”€â”€â”€â”‚   API Gateway   â”‚â”€â”€â”€â”€â”‚   Web Dashboard â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Nexus Core    â”‚
                       â”‚  (Rust Service) â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Model Server â”‚    â”‚   Cache Layer    â”‚    â”‚  Metrics Store  â”‚
â”‚   (ONNX/HF)  â”‚    â”‚    (Redis)       â”‚    â”‚  (Prometheus)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

- **Core**: Rust (Tokio async runtime)
- **Web Framework**: Axum
- **Model Inference**: Candle ML, ONNX Runtime
- **Caching**: Redis
- **Metrics**: Prometheus + Grafana
- **Database**: PostgreSQL (for metadata)
- **Containerization**: Docker + Docker Compose
- **Configuration**: TOML/YAML based config

## ğŸ“¦ Project Structure

```
nexus/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/                 # REST API endpoints
â”‚   â”‚   â”œâ”€â”€ models.rs
â”‚   â”‚   â”œâ”€â”€ inference.rs
â”‚   â”‚   â””â”€â”€ metrics.rs
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ orchestrator.rs  # Main orchestration logic
â”‚   â”‚   â”œâ”€â”€ load_balancer.rs # Load balancing algorithms
â”‚   â”‚   â”œâ”€â”€ batcher.rs       # Request batching
â”‚   â”‚   â””â”€â”€ cache.rs         # Caching layer
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ manager.rs       # Model lifecycle management
â”‚   â”‚   â”œâ”€â”€ runner.rs        # Model inference execution
â”‚   â”‚   â””â”€â”€ registry.rs      # Model registry
â”‚   â”œâ”€â”€ observability/
â”‚   â”‚   â”œâ”€â”€ metrics.rs       # Prometheus metrics
â”‚   â”‚   â”œâ”€â”€ tracing.rs       # Distributed tracing
â”‚   â”‚   â””â”€â”€ health.rs        # Health checks
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ mod.rs           # Configuration management
â”‚   â””â”€â”€ main.rs
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ load/                # Load testing
â”‚   â””â”€â”€ unit/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ nexus.toml           # Main configuration
â”‚   â””â”€â”€ models.yaml          # Model definitions
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup.sh
â”‚   â””â”€â”€ benchmark.sh
â””â”€â”€ docs/
    â”œâ”€â”€ api.md
    â”œâ”€â”€ deployment.md
    â””â”€â”€ architecture.md
```

## ğŸš€ Quick Start

### Prerequisites

- Rust 1.75+ installed
- Docker and Docker Compose
- At least 8GB RAM (for model loading)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/nexus.git
   cd nexus
   ```

2. **Start infrastructure services**
   ```bash
   docker-compose up -d redis postgres prometheus grafana
   ```

3. **Build and run Nexus**
   ```bash
   cargo build --release
   ./target/release/nexus --config configs/nexus.toml
   ```

4. **Load your first model**
   ```bash
   curl -X POST http://localhost:8080/api/v1/models \
     -H "Content-Type: application/json" \
     -d '{
       "name": "gpt2-small",
       "model_type": "huggingface",
       "model_path": "gpt2",
       "max_batch_size": 8,
       "max_sequence_length": 512
     }'
   ```

### Usage Examples

**Text Generation**
```bash
curl -X POST http://localhost:8080/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt2-small",
    "prompt": "The future of AI is",
    "max_tokens": 50,
    "temperature": 0.7
  }'
```

**Batch Inference**
```bash
curl -X POST http://localhost:8080/api/v1/generate/batch \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt2-small",
    "prompts": [
      "Hello, how are you?",
      "What is machine learning?",
      "Explain quantum computing"
    ],
    "max_tokens": 100
  }'
```

## ğŸ“Š Key Components

### 1. Orchestrator Engine
The core orchestration engine handles:
- Request routing and prioritization
- Model selection based on load and performance
- Graceful error handling and fallback strategies

### 2. Dynamic Batching System
- Intelligent batching based on model capacity
- Timeout-based batch flushing
- Priority-aware request scheduling

### 3. Multi-layered Caching
- **L1**: In-memory cache for frequent requests
- **L2**: Redis-based distributed cache
- **L3**: Model-specific result caching with TTL

### 4. Observability Suite
- Request latency and throughput metrics
- Model performance tracking
- Resource utilization monitoring
- Custom alerting rules

### 5. A/B Testing Framework
- Traffic splitting capabilities
- Statistical significance testing
- Performance comparison dashboards

## âš¡ Performance Features

- **Zero-copy serialization** with efficient memory management
- **Concurrent request processing** with Tokio async runtime
- **Model warmup strategies** for reduced cold start latency
- **Adaptive batching** that scales with load
- **Connection pooling** for database and cache operations

## ğŸ”§ Configuration

### Model Configuration (`models.yaml`)
```yaml
models:
  - name: "gpt2-small"
    type: "huggingface"
    path: "gpt2"
    device: "cpu"
    max_batch_size: 8
    warmup: true
    health_check_interval: 30s
    
  - name: "custom-model"
    type: "onnx"
    path: "./models/custom.onnx"
    device: "cuda:0"
    max_batch_size: 16
    preprocessing: "tokenize"
```

### Nexus Configuration (`nexus.toml`)
```toml
[server]
host = "0.0.0.0"
port = 8080
workers = 4

[cache]
redis_url = "redis://localhost:6379"
ttl_seconds = 300
max_size_mb = 1024

[metrics]
prometheus_port = 9090
collection_interval = "10s"

[load_balancer]
algorithm = "least_connections"
health_check_interval = "5s"
failure_threshold = 3
```

## ğŸ“ˆ Monitoring & Observability

### Metrics Dashboard
Access Grafana at `http://localhost:3000` with default credentials (admin/admin):
- Request rate and latency percentiles
- Model performance comparisons
- Resource utilization trends
- Error rate tracking

### Health Endpoints
- `GET /health` - Overall system health
- `GET /health/models` - Individual model status
- `GET /metrics` - Prometheus metrics endpoint

## ğŸ§ª Testing & Benchmarking

### Run the test suite
```bash
# Unit tests
cargo test

# Integration tests
cargo test --test integration

# Load testing (requires Python + locust)
cd tests/load && locust -f load_test.py
```

### Performance Benchmarks
```bash
# Run included benchmarks
./scripts/benchmark.sh

# Expected performance on modern hardware:
# - Single request latency: <50ms
# - Throughput: >1000 requests/second
# - Batch processing: 8x improvement over individual requests
```

## ğŸš¢ Deployment

### Docker Deployment
```bash
# Build production image
docker build -t nexus:latest .

# Deploy with compose
docker-compose -f docker/docker-compose.prod.yml up -d
```

### Kubernetes Deployment
```bash
# Deploy to Kubernetes
kubectl apply -f k8s/
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with tests
4. Commit your changes (`git commit -m 'Add amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

### Development Setup
```bash
# Install development dependencies
rustup component add rustfmt clippy
cargo install cargo-watch

# Run in development mode with hot reload
cargo watch -x run
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ† Key Achievements & Resume Highlights

This project demonstrates:
- **Advanced Rust Programming**: Async/await, zero-cost abstractions, memory safety
- **Distributed Systems**: Load balancing, caching, horizontal scaling
- **MLOps Expertise**: Model serving, A/B testing, performance optimization
- **Production Engineering**: Monitoring, health checks, graceful degradation
- **Performance Engineering**: Sub-50ms latency, >1000 RPS throughput
- **DevOps Skills**: Docker, Kubernetes, CI/CD pipelines
- **System Architecture**: Microservices, event-driven design, observability

## ğŸ“š Learning Resources

- [Rust Async Book](https://rust-lang.github.io/async-book/)
- [Candle ML Framework](https://github.com/huggingface/candle)
- [Prometheus Rust Client](https://docs.rs/prometheus/latest/prometheus/)
- [Axum Web Framework](https://docs.rs/axum/latest/axum/)

---

**Built by Abdulrahman Mahmoud**